---
title: "MTH210: Lab 7"
format: 
  pdf:
    documentclass: article
editor: visual
geometry: margin=1in
header-includes:
   - \usepackage{amsmath, amssymb, setspace}
   - \onehalfspacing
   - \usepackage{etoolbox} 
   - \makeatletter 
   - \preto{\@verbatim}{\topsep=3pt \partopsep=3pt } 
   - \makeatother
---

## Likelihood functions

1.  Suppose $X_1, X_2 \overset{iid}{\sim} N(\theta, 1)$, and you observed $X_1 = x_1 = 2$ and $X_2 = x_2 = 3$. For various values of $\theta$, draw the likelihood function of $\theta$. Recall that

    $$
    L(\theta | \mathbf{X}) = f(x_1 | \theta) f(x_2 | \theta)\,,
    $$

    where $f(x | \theta)$ is the density of a $N(\theta, 1)$ distribution evaluated at the value $x$. So you have to find this product and then choose a grid of values of $\theta$ on the x-axis and plot the corresponding value of $L(\theta | x_1 = 2, x_2 = 3)$.

2.  Suppose you obtain $X_1, X_2, \dots, X_{100} \overset{iid}{\sim}$ Gamma$(\alpha, 2)$, with $\alpha = 10$. For various values of $\alpha$, draw the likelihood function $L(\alpha | \mathbf{X})$ and draw log of the likelihood function: $\log L(\alpha | \mathbf{X})$. "Verify" that both $L(\alpha | \mathbf{X})$ and $\log L(\alpha|\mathbf{X})$ have the same maxima.

3.  Our goal in this next problem is to draw the likelihood function as $n$, the data size increases. Consider the N$(\theta,1)$ distribution. Suppose we obtain one data-point from this distribution and calculate the log-likelihood:

    $$
    \log L(\theta | X_1) = \log f(X_1 | \theta),. 
    $$

    Every time we obtain different data points, we will get different log-likelihood functions. Thus, the log-likelihood function is random, where the source of randomness is the data.

    Similarly, suppose we get now $n = 100$ data points from $N(\theta,1)$. Here again, we obtain the log-likelihood

    $$
    \log L(\theta | X_1, \dots, X_n) =  \sum_{i=1}^{n} \log f(X_i | \theta),. 
    $$

    For both situations, in file `demonstration.R` we will plot

    $$
    \frac{\text{Log-likelihood}}{n}\,.
    $$

    Notice that

    $$
    \hat{\theta}_{MLE} = \arg \max \{\text{Log-likelihood}\} =  \arg \max \frac{\text{Log-likelihood}}{n}
    $$

    In the demonstration, we can see what happens to the MLE estimator as $n$ increases. Now, change the code in the demonstration to increase $n$ to $500$ to see how this affects the MLE.
